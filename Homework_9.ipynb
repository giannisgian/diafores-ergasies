{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework 9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giannisgian/diafores-ergasies/blob/main/Homework_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcsjgilL58Qo",
        "outputId": "44b5d088-27dd-4108-8fff-91144ea88069",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Twython in /usr/local/lib/python3.7/dist-packages (3.9.1)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from Twython) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from Twython) (1.3.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->Twython) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->Twython) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->Twython) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->Twython) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.4.0->Twython) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install Twython\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "from twython import Twython\n",
        "from twython import TwythonError\n",
        "\n",
        "\n",
        "\n",
        "CONSUMER_KEY = \"9mfZ8EE6sWFNWKL8DWf2ljJd7\"\n",
        "CONSUMER_SECRET = \"JcjmNduWkc0If8rJHvoUKwCDXPNm69mUo0EPBfBo9XZxXT7LHw\"\n",
        "OAUTH_TOKEN = \"784496468455526402-uJVsC0VdifpK2dQTfniRV3m0pWI5Xgp\"\n",
        "OAUTH_TOKEN_SECRET = \"PphjBKgPE2sTf567uDknSaUSG4xU6HDokaQCUUy9n5JXH\"\n",
        "twitter = Twython(\n",
        "    CONSUMER_KEY, CONSUMER_SECRET,\n",
        "    OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Δημοσιογράφος 1"
      ],
      "metadata": {
        "id": "P42toE07TaSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweetsL = []\n",
        "try:\n",
        "user_timeline = twitter.get_user_timeline(screen_name='atsipras',count=150, tweet_mode = 'extended'  )\n",
        "except TwythonError as e:\n",
        "    print(\"Error getting tweets:\", e)\n",
        "print(\"I Got:\", len(user_timeline), \" tweets\")\n",
        "for tweet in user_timeline:\n",
        "    # Add whatever you want from the tweet, here we just add the text\n",
        "    tweetsL.append(tweet)"
      ],
      "metadata": {
        "id": "yI9M58Gf6Ae9",
        "outputId": "8dbf0c93-6fd3-4a36-a368-fa553d2cc286",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-7b9e71559c0f>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    user_timeline = twitter.get_user_timeline(screen_name='atsipras',count=150, tweet_mode = 'extended'  )\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count could be less than 200, see:\n",
        "# https://dev.twitter.com/discussions/7513\n",
        "while len(user_timeline) != 0: \n",
        "    try:\n",
        "        user_timeline = twitter.get_user_timeline(screen_name='',count=100,\n",
        "                                                  tweet_mode = 'extended', max_id=user_timeline[len(user_timeline)-1]['id']-1)\n",
        "    except TwythonError as e:\n",
        "        print(\"Error getting tweets:\", e)\n",
        "    if len(user_timeline) > 0:\n",
        "        print(\"I Got:\", len(user_timeline), \" tweets more... Last ID:\", user_timeline[len(user_timeline)-1]['id']-1)\n",
        "    for tweet in user_timeline:\n",
        "        # Add whatever you want from the tweet, here we just add the text\n",
        "        tweetsL.append(tweet)\n",
        "# Number of tweets the user has made\n",
        "print(\"I got :\", len(tweetsL), \" in total!!!\")"
      ],
      "metadata": {
        "id": "ikJxG5bh6E3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#len(tweetsL)\n"
      ],
      "metadata": {
        "id": "0VvB6bSz6aMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tweet in tweetsL:\n",
        "    print(tweet['created_at'],tweet['full_text'])"
      ],
      "metadata": {
        "id": "peFNtR6q6cOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweetsL[0]['full_text']"
      ],
      "metadata": {
        "id": "drZlvkBL6enE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweetsL[0]"
      ],
      "metadata": {
        "id": "xVCeoxqB6gv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df = pd.DataFrame(tweetsL)"
      ],
      "metadata": {
        "id": "EJ7rrmoC6isJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df.head()"
      ],
      "metadata": {
        "id": "jLQH6n6K6kjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "lEsAR6Rn6mMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df.to_csv(\"/content/gdrive/My Drive/Colab Notebooks/.tsv\", sep='\\t')"
      ],
      "metadata": {
        "id": "SoSCIpAo6qDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "short_tweets = df[['created_at','full_text' ]]\n",
        "short_tweets = short_tweets.rename(columns={'created_at': 'date', 'full_text': 'text'} )\n",
        "short_tweets"
      ],
      "metadata": {
        "id": "aihnFyk58Gjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#short_tweets.dtypes\n",
        "short_tweets['date'] = pd.to_datetime(short_tweets['date'], format='%a %b %d %H:%M:%S +0000 %Y')\n",
        "short_tweets"
      ],
      "metadata": {
        "id": "4sah7G3NDS43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "short_tweets['year'] = pd.DatetimeIndex(short_tweets['date']).year\n",
        "short_tweets['month'] = pd.DatetimeIndex(short_tweets['date']).month\n",
        "short_tweets['day'] = pd.DatetimeIndex(short_tweets['date']).day\n",
        "short_tweets"
      ],
      "metadata": {
        "id": "86ZgKO_QDeal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "short_tweets['text'] = short_tweets['text'].str.replace(r'https?:\\/\\/.*[\\r\\n]*',\" \")"
      ],
      "metadata": {
        "id": "U8uxyQDkDwsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "HPVHYjVdD2Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "short_tweets[short_tweets['year']==2022]['text']"
      ],
      "metadata": {
        "id": "TkYHt8o8D3KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = short_tweets[short_tweets['year']==2022]['text'].str.cat(sep = '.').replace(\"amp\", ' ')\n",
        "text"
      ],
      "metadata": {
        "id": "Z15YY5rCD8VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(STOPWORDS)[0:20]"
      ],
      "metadata": {
        "id": "MNBOurDtEBff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud = WordCloud(\n",
        "#   stopwords = STOPWORDS,\n",
        "    width = 2000,\n",
        "    height = 1000,\n",
        "    background_color = 'black'\n",
        " ).generate(text)\n",
        "fig = plt.figure(\n",
        "    figsize = (40, 30),\n",
        "    facecolor = 'k',\n",
        "    edgecolor = 'k')\n",
        "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
        "plt.axis('off')\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Csb2wKn2EEPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "!python -m spacy download el_core_news_sm"
      ],
      "metadata": {
        "id": "qGfJ85tIEIz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "vt2gVb57ELwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(nlp.Defaults.stop_words)[0:20]"
      ],
      "metadata": {
        "id": "K4cGNQdmEOQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud = WordCloud(\n",
        "    stopwords = nlp.Defaults.stop_words, \n",
        "    width = 2000,\n",
        "    height = 1000,\n",
        "    background_color = 'black'\n",
        " ).generate(text)\n",
        "fig = plt.figure(\n",
        "    figsize = (40, 30),\n",
        "    facecolor = 'k',\n",
        "    edgecolor = 'k')\n",
        "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
        "plt.axis('off')\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JYh4trTcEQPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()"
      ],
      "metadata": {
        "id": "ZG-9Z5X4ETGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vector = cv.fit_transform(short_tweets['text'])cv.get_feature_names()\n",
        "\n",
        "results = pd.DataFrame(count_vector.toarray(), columns=cv.get_feature_names()) \n",
        "\n",
        "results.head()\n",
        "count_vector.shape"
      ],
      "metadata": {
        "id": "vN1zKUBmEu0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cv.vocabulary_\n"
      ],
      "metadata": {
        "id": "WKrT1GtiEyIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(stop_words= nlp.Defaults.stop_words, min_df=0.01, max_df=0.95)"
      ],
      "metadata": {
        "id": "hG27Z9zNE347"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vector = cv.fit_transform(short_tweets['text'])"
      ],
      "metadata": {
        "id": "W5CgxyE6E6Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame(count_vector.toarray(), columns=cv.get_feature_names()) \n",
        "\n",
        "results.head()"
      ],
      "metadata": {
        "id": "qlKRja2aE9Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from html import unescape\n",
        "def my_tokenizer(doc):\n",
        "            \n",
        "    # apply the preprocessing and tokenzation steps\n",
        "    doc_clean = unescape(doc).lower()\n",
        "    tokens = nlp(doc_clean)\n",
        "    lemmatized_tokens = [token.lemma_ for token in tokens if (len(str(token.lemma_))>2)]\n",
        "            \n",
        "    # use CountVectorizer's _word_ngrams built in method\n",
        "    # to remove stop words and extract n-grams\n",
        "    return(lemmatized_tokens)"
      ],
      "metadata": {
        "id": "aH11xzDRFAFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(stop_words= nlp.Defaults.stop_words, min_df=0.01, max_df=0.95, tokenizer=my_tokenizer)\n",
        " count_vector = cv.fit_transform(short_tweets['text'])\n",
        " results = pd.DataFrame(count_vector.toarray(), columns=cv.get_feature_names()) \n",
        " results.head()"
      ],
      "metadata": {
        "id": "Dm4e2J-NFDrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer( stop_words= nlp.Defaults.stop_words, max_features=20, token_pattern = '\\\\bκυ[α-ωά-ώ]*\\\\b') #, tokenizer=my_tokenizer)\n",
        "count_vector = cv.fit_transform(short_tweets['text'])\n",
        "results = pd.DataFrame(count_vector.toarray(), columns=cv.get_feature_names()) \n",
        "results.head()"
      ],
      "metadata": {
        "id": "8_NG0yeaFIYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer( stop_words= nlp.Defaults.stop_words, max_features=20, ngram_range=(2, 2) ) #token_pattern = '\\\\bκυ[α-ωά-ώ]*\\\\b') #, tokenizer=my_tokenizer)\n",
        "count_vector = cv.fit_transform(short_tweets['text'])\n",
        "results = pd.DataFrame(count_vector.toarray(), columns=cv.get_feature_names()) \n",
        "results.head()"
      ],
      "metadata": {
        "id": "GCQAw20JFMKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.sum(axis =0).sort_values(ascending = False)"
      ],
      "metadata": {
        "id": "MNxDnHrjFReV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum_words =  results.sum(axis =0)\n",
        "sum_words.sort_values(ascending = False).head()  "
      ],
      "metadata": {
        "id": "LcRbwV6ZFUHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer( stop_words= nlp.Defaults.stop_words, max_features=20, #ngram_range=(1, 2), \n",
        " #                    token_pattern = '(\\\\bκ[οω]ρ[οω]ν[α-ωά-ώ]+\\\\b') )\\| (\\\\bκυβ[α-ωά-ώ]\\\\b) #, tokenizer=my_tokenizer)\n",
        "                     token_pattern = '\\\\bukr[a-z0-9-_]*\\\\b') #, tokenizer=my_tokenizer)\n",
        "count_vector = cv.fit_transform(short_tweets['text'])\n",
        "results = pd.DataFrame(count_vector.toarray(), columns=cv.get_feature_names()) \n",
        "results.head()"
      ],
      "metadata": {
        "id": "GdH-ZRWEFXbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.sum(axis =0).sort_values(ascending = False)"
      ],
      "metadata": {
        "id": "NLCKrlz5FZMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer( stop_words= nlp.Defaults.stop_words, max_features=20, #ngram_range=(1, 2), \n",
        "                     vocabulary = ['ukraine', 'ουκρανία']) #, tokenizer=my_tokenizer)\n",
        "count_vector = cv.fit_transform(short_tweets['text'])\n",
        "results = pd.DataFrame(count_vector.toarray(), columns=cv.get_feature_names()) \n",
        "results.head()"
      ],
      "metadata": {
        "id": "PPQdPi-RCkA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "Zx5_6z9qFihB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(stop_words= nlp.Defaults.stop_words, max_features=500)\n",
        "tfidf_matrix_train = tfidf_vectorizer.fit_transform(short_tweets['text'])  #finds the tfidf score with normalization"
      ],
      "metadata": {
        "id": "apIUNO0TDdwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(\"cosine scores ==> \",cosine_similarity(tfidf_matrix_train[0:1], tfidf_matrix_train))  #here the first element of tfidf_matrix_train is matched with oth"
      ],
      "metadata": {
        "id": "yYGSCxTGEKU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"cosine scores ==> \",cosine_similarity(tfidf_matrix_train, tfidf_matrix_train))  #here the first element of tfidf_matrix_train is matched with other three elements\n"
      ],
      "metadata": {
        "id": "5JR3xql-Fw5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = cosine_similarity(tfidf_matrix_train, tfidf_matrix_train)"
      ],
      "metadata": {
        "id": "RbPGzKvSFzga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(arr).loc[3].sort_values(ascending = False).head(20).plot(kind = 'bar')\n"
      ],
      "metadata": {
        "id": "SJgyAQg-F0dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "short_tweets['text'].loc[3]"
      ],
      "metadata": {
        "id": "9bpgLO7mF2ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "short_tweets['text'].loc[2823]"
      ],
      "metadata": {
        "id": "MGOCP7G-F8yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Δημοσιογράφος 2"
      ],
      "metadata": {
        "id": "Kjdr1xmEThzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweetsL = []\n",
        "try:\n",
        "    user_timeline = twitter.get_user_timeline(screen_name='',count=100, tweet_mode = 'extended'  )\n",
        "except TwythonError as e:\n",
        "    print(\"Error getting tweets:\", e)\n",
        "print(\"I Got:\", len(user_timeline), \" tweets\")\n",
        "for tweet in user_timeline:\n",
        "    # Add whatever you want from the tweet, here we just add the text\n",
        "    tweetsL.append(tweet)"
      ],
      "metadata": {
        "id": "AhGgq5a6ThzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count could be less than 200, see:\n",
        "# https://dev.twitter.com/discussions/7513\n",
        "while len(user_timeline) != 0: \n",
        "    try:\n",
        "        user_timeline = twitter.get_user_timeline(screen_name='',count=100,\n",
        "                                                  tweet_mode = 'extended', max_id=user_timeline[len(user_timeline)-1]['id']-1)\n",
        "    except TwythonError as e:\n",
        "        print(\"Error getting tweets:\", e)\n",
        "    if len(user_timeline) > 0:\n",
        "        print(\"I Got:\", len(user_timeline), \" tweets more... Last ID:\", user_timeline[len(user_timeline)-1]['id']-1)\n",
        "    for tweet in user_timeline:\n",
        "        # Add whatever you want from the tweet, here we just add the text\n",
        "        tweetsL.append(tweet)\n",
        "# Number of tweets the user has made\n",
        "print(\"I got :\", len(tweetsL), \" in total!!!\")"
      ],
      "metadata": {
        "id": "3-xfnvm4Thza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#len(tweetsL)\n"
      ],
      "metadata": {
        "id": "dKicTuPUThza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tweet in tweetsL:\n",
        "    print(tweet['created_at'],tweet['full_text'])"
      ],
      "metadata": {
        "id": "0DExoBG_Thzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweetsL[0]['full_text']"
      ],
      "metadata": {
        "id": "_JMhD5fkThzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweetsL[0]"
      ],
      "metadata": {
        "id": "cF9IlXo5Thzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df = pd.DataFrame(tweetsL)"
      ],
      "metadata": {
        "id": "zOaMcqG5Thzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df.head()"
      ],
      "metadata": {
        "id": "jRxN73uqThzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "0rR74nC-Thzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df.to_csv(\"/content/gdrive/My Drive/Colab Notebooks/.tsv\", sep='\\t')"
      ],
      "metadata": {
        "id": "fr5KHboNThzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "short_tweets = df[['created_at','full_text' ]]\n",
        "short_tweets = short_tweets.rename(columns={'created_at': 'date', 'full_text': 'text'} )\n",
        "short_tweets"
      ],
      "metadata": {
        "id": "KlLF1fDBThzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#short_tweets.dtypes\n",
        "short_tweets['date'] = pd.to_datetime(short_tweets['date'], format='%a %b %d %H:%M:%S +0000 %Y')\n",
        "short_tweets"
      ],
      "metadata": {
        "id": "M8Fd8LVMThzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "short_tweets['year'] = pd.DatetimeIndex(short_tweets['date']).year\n",
        "short_tweets['month'] = pd.DatetimeIndex(short_tweets['date']).month\n",
        "short_tweets['day'] = pd.DatetimeIndex(short_tweets['date']).day\n",
        "short_tweets"
      ],
      "metadata": {
        "id": "4qay5GcsThzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "short_tweets['text'] = short_tweets['text'].str.replace(r'https?:\\/\\/.*[\\r\\n]*',\" \")"
      ],
      "metadata": {
        "id": "c19jc2EpThzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Rl2H6PmkThzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "short_tweets[short_tweets['year']==2022]['text']"
      ],
      "metadata": {
        "id": "WVfqR7XuThzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = short_tweets[short_tweets['year']==2022]['text'].str.cat(sep = '.').replace(\"amp\", ' ')\n",
        "text"
      ],
      "metadata": {
        "id": "MTQus7TKThzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(STOPWORDS)[0:20]"
      ],
      "metadata": {
        "id": "IZYhwEzhThzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud = WordCloud(\n",
        "#   stopwords = STOPWORDS,\n",
        "    width = 2000,\n",
        "    height = 1000,\n",
        "    background_color = 'black'\n",
        " ).generate(text)\n",
        "fig = plt.figure(\n",
        "    figsize = (40, 30),\n",
        "    facecolor = 'k',\n",
        "    edgecolor = 'k')\n",
        "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
        "plt.axis('off')\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A9w66KWvThze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "!python -m spacy download el_core_news_sm"
      ],
      "metadata": {
        "id": "dmlscXTmThze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "JaJLkJ8ZThze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(nlp.Defaults.stop_words)[0:20]"
      ],
      "metadata": {
        "id": "6c9azcJyThze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud = WordCloud(\n",
        "    stopwords = nlp.Defaults.stop_words, \n",
        "    width = 2000,\n",
        "    height = 1000,\n",
        "    background_color = 'black'\n",
        " ).generate(text)\n",
        "fig = plt.figure(\n",
        "    figsize = (40, 30),\n",
        "    facecolor = 'k',\n",
        "    edgecolor = 'k')\n",
        "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
        "plt.axis('off')\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xHjgUFY8Thze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()"
      ],
      "metadata": {
        "id": "Bc01zePxThze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vector = cv.fit_transform(short_tweets['text'])cv.get_feature_names()\n",
        "\n",
        "results = pd.DataFrame(count_vector.toarray(), columns=cv.get_feature_names()) \n",
        "\n",
        "results.head()\n",
        "count_vector.shape"
      ],
      "metadata": {
        "id": "Yn5qclLdThzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cv.vocabulary_\n"
      ],
      "metadata": {
        "id": "moPktGdKThzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(stop_words= nlp.Defaults.stop_words, min_df=0.01, max_df=0.95)"
      ],
      "metadata": {
        "id": "gpok2aJOThzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vector = cv.fit_transform(short_tweets['text'])"
      ],
      "metadata": {
        "id": "BWcPciyEThzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame(count_vector.toarray(), columns=cv.get_feature_names()) \n",
        "\n",
        "results.head()"
      ],
      "metadata": {
        "id": "XAIWTy8aThzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from html import unescape\n",
        "def my_tokenizer(doc):\n",
        "            \n",
        "    # apply the preprocessing and tokenzation steps\n",
        "    doc_clean = unescape(doc).lower()\n",
        "    tokens = nlp(doc_clean)\n",
        "    lemmatized_tokens = [token.lemma_ for token in tokens if (len(str(token.lemma_))>2)]\n",
        "            \n",
        "    # use CountVectorizer's _word_ngrams built in method\n",
        "    # to remove stop words and extract n-grams\n",
        "    return(lemmatized_tokens)"
      ],
      "metadata": {
        "id": "U24ssgxEThzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(stop_words= nlp.Defaults.stop_words, min_df=0.01, max_df=0.95, tokenizer=my_tokenizer)\n",
        " count_vector = cv.fit_transform(short_tweets['text'])\n",
        " results = pd.DataFrame(count_vector.toarray(), columns=cv.get_feature_names()) \n",
        " results.head()"
      ],
      "metadata": {
        "id": "nWc4b9JkThzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer( stop_words= nlp.Defaults.stop_words, max_features=20, token_pattern = '\\\\bκυ[α-ωά-ώ]*\\\\b') #, tokenizer=my_tokenizer)\n",
        "count_vector = cv.fit_transform(short_tweets['text'])\n",
        "results = pd.DataFrame(count_vector.toarray(), columns=cv.get_feature_names()) \n",
        "results.head()"
      ],
      "metadata": {
        "id": "FmefvKuYThzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer( stop_words= nlp.Defaults.stop_words, max_features=20, ngram_range=(2, 2) ) #token_pattern = '\\\\bκυ[α-ωά-ώ]*\\\\b') #, tokenizer=my_tokenizer)\n",
        "count_vector = cv.fit_transform(short_tweets['text'])\n",
        "results = pd.DataFrame(count_vector.toarray(), columns=cv.get_feature_names()) \n",
        "results.head()"
      ],
      "metadata": {
        "id": "J-2M7j_rThzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.sum(axis =0).sort_values(ascending = False)"
      ],
      "metadata": {
        "id": "SxDBiVE4Thzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum_words =  results.sum(axis =0)\n",
        "sum_words.sort_values(ascending = False).head()  "
      ],
      "metadata": {
        "id": "__-XPuBvThzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer( stop_words= nlp.Defaults.stop_words, max_features=20, #ngram_range=(1, 2), \n",
        " #                    token_pattern = '(\\\\bκ[οω]ρ[οω]ν[α-ωά-ώ]+\\\\b') )\\| (\\\\bκυβ[α-ωά-ώ]\\\\b) #, tokenizer=my_tokenizer)\n",
        "                     token_pattern = '\\\\bukr[a-z0-9-_]*\\\\b') #, tokenizer=my_tokenizer)\n",
        "count_vector = cv.fit_transform(short_tweets['text'])\n",
        "results = pd.DataFrame(count_vector.toarray(), columns=cv.get_feature_names()) \n",
        "results.head()"
      ],
      "metadata": {
        "id": "7_o0TCujThzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.sum(axis =0).sort_values(ascending = False)"
      ],
      "metadata": {
        "id": "-CugWTEOThzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer( stop_words= nlp.Defaults.stop_words, max_features=20, #ngram_range=(1, 2), \n",
        "                     vocabulary = ['ukraine', 'ουκρανία']) #, tokenizer=my_tokenizer)\n",
        "count_vector = cv.fit_transform(short_tweets['text'])\n",
        "results = pd.DataFrame(count_vector.toarray(), columns=cv.get_feature_names()) \n",
        "results.head()"
      ],
      "metadata": {
        "id": "TX_UifLsThzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "_Rw8O5KvThzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(stop_words= nlp.Defaults.stop_words, max_features=500)\n",
        "tfidf_matrix_train = tfidf_vectorizer.fit_transform(short_tweets['text'])  #finds the tfidf score with normalization"
      ],
      "metadata": {
        "id": "UvGEKGK4Thzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(\"cosine scores ==> \",cosine_similarity(tfidf_matrix_train[0:1], tfidf_matrix_train))  #here the first element of tfidf_matrix_train is matched with oth"
      ],
      "metadata": {
        "id": "4V-j61soThzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"cosine scores ==> \",cosine_similarity(tfidf_matrix_train, tfidf_matrix_train))  #here the first element of tfidf_matrix_train is matched with other three elements\n"
      ],
      "metadata": {
        "id": "SS-YX3plThzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = cosine_similarity(tfidf_matrix_train, tfidf_matrix_train)"
      ],
      "metadata": {
        "id": "mVhNShYNThzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(arr).loc[3].sort_values(ascending = False).head(20).plot(kind = 'bar')\n"
      ],
      "metadata": {
        "id": "d2vtYWKFThzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "short_tweets['text'].loc[3]"
      ],
      "metadata": {
        "id": "5MLg2vrTThzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "short_tweets['text'].loc[2823]"
      ],
      "metadata": {
        "id": "R-mwXTRgThzh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}